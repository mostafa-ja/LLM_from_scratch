{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOhpOgkO5i2mRQKSg8yDkat",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mostafa-ja/Let-s-build-an-LLM/blob/main/Let's_build_GPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to build the model based on [karpathy's tutorial video ](https://www.youtube.com/watch?v=l8pRSuU81PU)\n",
        "\n",
        "More explanation also [here](https://devshahs.medium.com/build-gpt-with-me-implementing-gpt-from-scratch-step-by-step-b2efe4e2f7e0), [here ](https://medium.com/@tusharmadaan/reproducing-gpt-2-124m-key-insights-and-techniques-cd8899864bbe) and [here ](https://towardsdatascience.com/line-by-line-lets-reproduce-gpt-2-section-1-b26684f98492/)\n",
        "\n",
        "Tokenizer for GPT [here ](https://tiktokenizer.vercel.app/)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ME3Azq8E2Uiq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import data"
      ],
      "metadata": {
        "id": "1Q2ibZ0KdLpw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's download the tiny shakespeare dataset\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPhTFskSDVJc",
        "outputId": "d09aae77-9e86-40a5-e468-77be6af2d2d9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-26 13:09:24--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.009s  \n",
            "\n",
            "2025-03-26 13:09:24 (114 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read it in to inspect it\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(\"length of dataset in characters: \", len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82H16s3rd8Tt",
        "outputId": "72b218d9-eefd-4c53-c225-db9174f84b40"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters:  1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's look at the first 200 characters\n",
        "print(text[:200])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPBNj-JBeFBF",
        "outputId": "e32f1a9b-8672-4444-8942-8c440f511cf3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GeE432S5kkUU",
        "outputId": "9de3f58d-8d25-48e3-b800-8c4b21f0f8b3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lower_text = text.lower()\n",
        "print(lower_text[:200])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSoiLrIWmEzw",
        "outputId": "b28abc62-874d-45c0-d0ef-e6021739245b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "first citizen:\n",
            "before we proceed any further, hear me speak.\n",
            "\n",
            "all:\n",
            "speak, speak.\n",
            "\n",
            "first citizen:\n",
            "you are all resolved rather to die than to famish?\n",
            "\n",
            "all:\n",
            "resolved. resolved.\n",
            "\n",
            "first citizen:\n",
            "first, you\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# here are all the unique characters that occur in this text\n",
        "lower_chars = sorted(list(set(lower_text)))\n",
        "lower_vocab_size = len(lower_chars)\n",
        "print(''.join(lower_chars))\n",
        "print(lower_vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHtce_i5mLMS",
        "outputId": "b50a9278-20ad-4f62-a88a-2affe5dad787"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?abcdefghijklmnopqrstuvwxyz\n",
            "39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Open a file in write mode and save the text\n",
        "with open(\"lower_input.txt\", \"w\", encoding=\"utf-8\") as file:\n",
        "    file.write(lower_text)\n",
        "\n",
        "print(\"lower_input saved successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gy6zBAdfmo3w",
        "outputId": "81beed77-598f-4a38-f9c8-565f9b21998b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lower_input saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "ky-wkmk-kxOp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "# train a sentencepiece model on it\n",
        "# the settings here are (best effort) those used for training Llama 2\n",
        "import os\n",
        "\n",
        "options = dict(\n",
        "  # input spec\n",
        "  input=\"input.txt\",\n",
        "  input_format=\"text\",\n",
        "  # output spec\n",
        "  model_prefix=\"tok512\", # output filename prefix\n",
        "  # algorithm spec\n",
        "  # BPE alg\n",
        "  model_type=\"bpe\",\n",
        "  vocab_size=512,\n",
        "  # normalization\n",
        "  normalization_rule_name=\"identity\", # ew, turn off normalization\n",
        "  remove_extra_whitespaces=False,\n",
        "  input_sentence_size=200000000, # max number of training sentences\n",
        "  max_sentence_length=4192, # max number of bytes per sentence\n",
        "  seed_sentencepiece_size=1000000,\n",
        "  shuffle_input_sentence=True,\n",
        "  # rare word treatment\n",
        "  character_coverage=0.99995,\n",
        "  byte_fallback=False,\n",
        "  # merge rules\n",
        "  split_digits=True,\n",
        "  split_by_unicode_script=True,\n",
        "  split_by_whitespace=True,\n",
        "  split_by_number=True,\n",
        "  max_sentencepiece_length=16,\n",
        "  add_dummy_prefix=True,\n",
        "  allow_whitespace_only_pieces=True,\n",
        "  # special tokens\n",
        "  unk_id=0, # the UNK token MUST exist\n",
        "  bos_id=1, # the others are optional, set to -1 to turn off\n",
        "  eos_id=2,\n",
        "  pad_id=-1,\n",
        "  # systems\n",
        "  num_threads=os.cpu_count(), # use ~all system resources\n",
        ")\n",
        "\n",
        "spm.SentencePieceTrainer.train(**options)\n"
      ],
      "metadata": {
        "id": "EgN7cEBNOcPe"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('tok512.model')\n",
        "vocab = [[sp.id_to_piece(idx), idx] for idx in range(sp.get_piece_size())]\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2SpdXfFk_i-",
        "outputId": "1522debe-4f30-4647-c26a-e1a8b398d530"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['<unk>', 0], ['<s>', 1], ['</s>', 2], ['▁t', 3], ['he', 4], ['▁a', 5], ['ou', 6], ['▁s', 7], ['▁m', 8], ['▁w', 9], ['in', 10], ['re', 11], ['ha', 12], ['▁the', 13], ['nd', 14], ['▁b', 15], ['is', 16], ['or', 17], ['▁f', 18], ['▁I', 19], ['er', 20], ['ll', 21], ['it', 22], ['on', 23], ['▁d', 24], ['▁c', 25], ['▁n', 26], ['▁l', 27], ['▁y', 28], ['es', 29], ['en', 30], ['▁th', 31], ['ar', 32], ['▁h', 33], ['▁o', 34], ['▁to', 35], ['▁you', 36], ['▁p', 37], ['▁T', 38], ['hat', 39], ['▁A', 40], ['▁he', 41], ['st', 42], ['ve', 43], ['ot', 44], ['▁and', 45], ['ow', 46], ['ing', 47], ['▁of', 48], ['an', 49], ['om', 50], ['▁g', 51], ['at', 52], ['▁be', 53], ['▁W', 54], ['se', 55], ['▁my', 56], ['▁in', 57], ['▁ha', 58], ['ce', 59], ['le', 60], ['ay', 61], ['ld', 62], ['ir', 63], ['et', 64], ['ed', 65], ['ut', 66], ['▁B', 67], ['▁me', 68], ['im', 69], ['▁S', 70], ['ith', 71], ['▁not', 72], ['▁H', 73], ['ch', 74], ['▁that', 75], ['▁is', 76], ['▁M', 77], ['gh', 78], ['▁And', 79], ['▁for', 80], ['▁u', 81], ['ke', 82], ['▁C', 83], ['▁we', 84], ['our', 85], ['oo', 86], ['ill', 87], ['▁e', 88], ['▁with', 89], ['her', 90], ['▁it', 91], ['ent', 92], ['▁your', 93], ['ad', 94], ['ri', 95], ['▁O', 96], ['▁thou', 97], ['▁st', 98], ['▁k', 99], ['▁L', 100], ['ome', 101], ['▁his', 102], ['▁F', 103], ['▁G', 104], ['ght', 105], ['EN', 106], ['ord', 107], ['▁re', 108], ['id', 109], ['ra', 110], ['▁The', 111], ['▁have', 112], ['▁him', 113], ['IN', 114], ['ly', 115], ['▁li', 116], ['as', 117], ['▁P', 118], ['▁this', 119], ['ur', 120], ['IO', 121], ['al', 122], ['▁so', 123], ['▁as', 124], ['▁de', 125], ['▁on', 126], ['▁R', 127], ['ore', 128], ['ro', 129], ['▁N', 130], ['hi', 131], ['ould', 132], ['ood', 133], ['AR', 134], ['ck', 135], ['ain', 136], ['ver', 137], ['▁Y', 138], ['est', 139], ['▁sha', 140], ['▁thy', 141], ['ess', 142], ['▁will', 143], ['▁do', 144], ['ea', 145], ['▁no', 146], ['am', 147], ['▁E', 148], ['▁but', 149], ['▁D', 150], ['us', 151], ['▁se', 152], ['US', 153], ['if', 154], [\"▁'\", 155], ['ge', 156], ['▁all', 157], ['and', 158], ['▁Th', 159], ['▁su', 160], ['ake', 161], ['▁To', 162], ['▁her', 163], ['ru', 164], ['ion', 165], ['▁an', 166], ['▁K', 167], ['▁lo', 168], ['ard', 169], ['ter', 170], ['han', 171], ['▁sp', 172], ['ell', 173], ['ear', 174], ['▁thee', 175], ['▁fa', 176], ['▁shall', 177], ['▁our', 178], ['▁by', 179], ['th', 180], ['UC', 181], ['▁are', 182], ['il', 183], ['ING', 184], ['▁ne', 185], ['▁kn', 186], ['rom', 187], ['ho', 188], ['▁v', 189], ['▁That', 190], ['ER', 191], ['OR', 192], ['ast', 193], ['ct', 194], ['ous', 195], ['▁what', 196], ['▁sh', 197], ['ight', 198], ['ul', 199], ['ET', 200], ['ant', 201], ['▁up', 202], ['sel', 203], ['▁good', 204], ['qu', 205], ['▁But', 206], ['art', 207], ['row', 208], ['ath', 209], ['ine', 210], ['▁com', 211], ['▁mu', 212], ['▁lord', 213], ['hich', 214], ['nt', 215], ['▁pr', 216], ['▁man', 217], ['▁at', 218], ['▁V', 219], ['one', 220], ['▁whe', 221], ['▁con', 222], ['▁What', 223], ['▁am', 224], ['end', 225], ['ic', 226], ['ES', 227], ['ble', 228], ['ry', 229], ['ong', 230], ['▁from', 231], ['ie', 232], ['▁she', 233], ['▁bl', 234], ['ive', 235], ['ven', 236], ['▁go', 237], ['▁For', 238], ['▁more', 239], ['▁them', 240], ['em', 241], ['▁was', 242], ['IC', 243], ['out', 244], ['au', 245], ['▁sir', 246], ['other', 247], ['are', 248], ['oth', 249], ['ol', 250], ['▁if', 251], ['▁He', 252], ['▁now', 253], ['▁hat', 254], ['▁there', 255], ['▁would', 256], ['AN', 257], ['ost', 258], ['▁know', 259], ['▁can', 260], ['LO', 261], ['ind', 262], ['ers', 263], ['IUS', 264], ['ep', 265], ['self', 266], ['ARD', 267], ['▁say', 268], ['▁KING', 269], ['ather', 270], ['ond', 271], ['ate', 272], ['res', 273], ['fe', 274], ['▁sw', 275], ['▁here', 276], ['▁love', 277], ['▁their', 278], ['▁My', 279], ['▁or', 280], ['▁than', 281], ['▁br', 282], ['pp', 283], ['--', 284], ['▁us', 285], ['▁king', 286], ['▁they', 287], ['▁then', 288], ['all', 289], ['▁ar', 290], ['od', 291], ['▁let', 292], ['▁un', 293], ['ig', 294], ['▁wor', 295], ['ure', 296], ['ink', 297], ['▁may', 298], ['hy', 299], ['ame', 300], ['▁come', 301], ['fore', 302], ['ort', 303], ['▁As', 304], ['▁qu', 305], ['ook', 306], ['▁well', 307], ['el', 308], ['ish', 309], ['LI', 310], ['▁Whe', 311], ['▁j', 312], ['KE', 313], ['ves', 314], ['▁one', 315], ['▁hath', 316], ['irst', 317], ['▁make', 318], ['▁You', 319], ['▁gra', 320], ['reat', 321], ['ak', 322], ['ng', 323], ['▁must', 324], ['gain', 325], ['ound', 326], ['▁were', 327], ['▁ho', 328], ['▁see', 329], ['▁should', 330], ['▁like', 331], ['ci', 332], ['▁pro', 333], ['ue', 334], ['eak', 335], ['TIO', 336], ['▁sa', 337], ['ity', 338], ['▁pl', 339], ['▁mad', 340], ['▁father', 341], ['▁had', 342], ['um', 343], ['▁did', 344], ['▁upon', 345], ['OM', 346], ['ime', 347], ['pe', 348], ['▁too', 349], ['ice', 350], ['ON', 351], ['▁J', 352], ['▁dis', 353], ['ence', 354], ['▁death', 355], ['ward', 356], ['▁which', 357], ['▁en', 358], ['▁If', 359], ['ose', 360], ['own', 361], ['▁speak', 362], ['▁po', 363], ['▁when', 364], ['ful', 365], ['▁out', 366], ['▁fri', 367], ['▁how', 368], ['entle', 369], ['io', 370], ['pt', 371], ['un', 372], ['ICH', 373], ['▁again', 374], ['▁RICH', 375], ['▁tru', 376], ['OL', 377], ['de', 378], ['UKE', 379], ['▁DUKE', 380], ['man', 381], ['▁heart', 382], ['▁yet', 383], ['ick', 384], ['▁hand', 385], ['very', 386], ['▁With', 387], ['▁RICHARD', 388], ['▁who', 389], ['ENTIO', 390], ['▁Thou', 391], ['ign', 392], ['▁We', 393], ['▁some', 394], ['ance', 395], ['▁Which', 396], ['▁hon', 397], ['▁mar', 398], ['nce', 399], ['▁r', 400], ['ies', 401], ['▁In', 402], ['▁son', 403], ['ire', 404], ['▁off', 405], ['▁mine', 406], ['ist', 407], ['INC', 408], ['▁al', 409], ['ince', 410], ['har', 411], ['orn', 412], ['ack', 413], ['▁Why', 414], ['wn', 415], ['▁Of', 416], ['▁God', 417], ['EL', 418], ['▁hea', 419], ['▁such', 420], ['▁time', 421], ['oy', 422], ['▁First', 423], ['▁hear', 424], ['▁So', 425], ['ness', 426], ['▁brother', 427], ['ec', 428], ['▁these', 429], ['▁Cl', 430], ['▁blood', 431], ['▁ab', 432], ['▁ex', 433], ['ither', 434], ['▁give', 435], ['UE', 436], ['▁fo', 437], ['▁Is', 438], ['▁No', 439], ['▁day', 440], ['▁ear', 441], ['▁ro', 442], ['▁look', 443], ['▁think', 444], ['▁II', 445], ['▁How', 446], ['▁life', 447], ['▁tell', 448], ['EST', 449], ['ise', 450], ['▁', 451], ['e', 452], ['t', 453], ['o', 454], ['a', 455], ['h', 456], ['s', 457], ['r', 458], ['n', 459], ['i', 460], ['l', 461], ['d', 462], ['u', 463], ['m', 464], ['y', 465], [',', 466], ['w', 467], ['f', 468], ['c', 469], ['g', 470], ['I', 471], ['b', 472], ['p', 473], [':', 474], ['.', 475], ['A', 476], ['v', 477], ['k', 478], ['T', 479], [\"'\", 480], ['E', 481], ['O', 482], ['N', 483], ['R', 484], ['S', 485], ['L', 486], ['C', 487], [';', 488], ['W', 489], ['U', 490], ['H', 491], ['M', 492], ['B', 493], ['?', 494], ['G', 495], ['!', 496], ['D', 497], ['-', 498], ['F', 499], ['Y', 500], ['P', 501], ['K', 502], ['V', 503], ['j', 504], ['q', 505], ['x', 506], ['z', 507], ['J', 508], ['Q', 509], ['Z', 510], ['X', 511]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "# train a sentencepiece model on it\n",
        "# the settings here are (best effort) those used for training Llama 2\n",
        "import os\n",
        "\n",
        "options = dict(\n",
        "  # input spec\n",
        "  input=\"lower_input.txt\",\n",
        "  input_format=\"text\",\n",
        "  # output spec\n",
        "  model_prefix=\"tok512_lower\", # output filename prefix\n",
        "  # algorithm spec\n",
        "  # BPE alg\n",
        "  model_type=\"bpe\",\n",
        "  vocab_size=512,\n",
        "  # normalization\n",
        "  normalization_rule_name=\"identity\", # ew, turn off normalization\n",
        "  remove_extra_whitespaces=False,\n",
        "  input_sentence_size=200000000, # max number of training sentences\n",
        "  max_sentence_length=4192, # max number of bytes per sentence\n",
        "  seed_sentencepiece_size=1000000,\n",
        "  shuffle_input_sentence=True,\n",
        "  # rare word treatment\n",
        "  character_coverage=0.99995,\n",
        "  byte_fallback=False,\n",
        "  # merge rules\n",
        "  split_digits=True,\n",
        "  split_by_unicode_script=True,\n",
        "  split_by_whitespace=True,\n",
        "  split_by_number=True,\n",
        "  max_sentencepiece_length=16,\n",
        "  add_dummy_prefix=True,\n",
        "  allow_whitespace_only_pieces=True,\n",
        "  # special tokens\n",
        "  unk_id=0, # the UNK token MUST exist\n",
        "  bos_id=1, # the others are optional, set to -1 to turn off\n",
        "  eos_id=2,\n",
        "  pad_id=-1,\n",
        "  # systems\n",
        "  num_threads=os.cpu_count(), # use ~all system resources\n",
        ")\n",
        "\n",
        "spm.SentencePieceTrainer.train(**options)"
      ],
      "metadata": {
        "id": "xBfgtz2ZmWNB"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sp_lower = spm.SentencePieceProcessor()\n",
        "sp_lower.load('tok512_lower.model')\n",
        "vocab_lower = [[sp_lower.id_to_piece(idx), idx] for idx in range(sp_lower.get_piece_size())]\n",
        "print(vocab_lower)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmnuFZQUnBgc",
        "outputId": "608ca16b-7d0f-425e-d82a-9512e8313789"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['<unk>', 0], ['<s>', 1], ['</s>', 2], ['▁t', 3], ['he', 4], ['▁a', 5], ['▁s', 6], ['▁w', 7], ['▁i', 8], ['ou', 9], ['▁m', 10], ['▁b', 11], ['ha', 12], ['re', 13], ['▁the', 14], ['nd', 15], ['in', 16], ['or', 17], ['▁f', 18], ['▁c', 19], ['▁o', 20], ['er', 21], ['▁l', 22], ['ll', 23], ['▁d', 24], ['▁n', 25], ['hi', 26], ['▁y', 27], ['en', 28], ['▁p', 29], ['st', 30], ['▁to', 31], ['▁and', 32], ['ar', 33], ['▁you', 34], ['▁he', 35], ['on', 36], ['▁g', 37], ['th', 38], ['es', 39], ['ing', 40], ['▁th', 41], ['▁of', 42], ['▁no', 43], ['hat', 44], ['an', 45], ['ve', 46], ['om', 47], ['▁wi', 48], ['▁be', 49], ['▁ha', 50], ['se', 51], ['▁my', 52], ['ce', 53], ['▁hi', 54], ['ow', 55], ['▁in', 56], ['at', 57], ['le', 58], ['▁that', 59], ['ri', 60], ['ed', 61], ['ay', 62], ['et', 63], ['▁for', 64], ['ut', 65], ['▁k', 66], ['ld', 67], ['▁is', 68], ['▁we', 69], ['ir', 70], ['us', 71], ['▁me', 72], ['▁e', 73], ['▁h', 74], ['ke', 75], ['▁not', 76], ['▁with', 77], ['ch', 78], ['gh', 79], ['oo', 80], ['ent', 81], ['it', 82], ['▁it', 83], ['▁your', 84], ['▁thou', 85], ['our', 86], ['her', 87], ['ea', 88], ['ro', 89], ['ome', 90], ['▁thi', 91], ['▁u', 92], ['▁st', 93], ['▁as', 94], ['is', 95], ['▁his', 96], ['▁but', 97], ['▁this', 98], ['▁so', 99], ['▁re', 100], ['▁li', 101], ['ad', 102], ['▁have', 103], ['ord', 104], ['▁v', 105], ['io', 106], ['ly', 107], ['ra', 108], ['ght', 109], ['▁on', 110], ['▁him', 111], ['al', 112], ['▁do', 113], ['as', 114], ['ck', 115], ['id', 116], ['▁what', 117], ['ur', 118], ['▁mo', 119], ['▁an', 120], ['▁sha', 121], ['ru', 122], ['▁se', 123], ['▁thy', 124], ['ould', 125], ['▁will', 126], ['ood', 127], ['am', 128], ['ain', 129], ['li', 130], ['ge', 131], ['▁king', 132], ['ess', 133], ['ard', 134], ['▁wh', 135], ['▁all', 136], ['and', 137], ['▁whe', 138], ['▁go', 139], ['▁her', 140], ['▁su', 141], ['▁by', 142], ['ther', 143], ['ver', 144], [\"▁'\", 145], ['▁lo', 146], ['▁shall', 147], ['ue', 148], ['▁fa', 149], ['▁lord', 150], ['ion', 151], ['▁ma', 152], ['▁our', 153], ['ear', 154], ['▁sp', 155], ['▁whi', 156], ['ell', 157], ['▁if', 158], ['▁are', 159], ['han', 160], ['▁thee', 161], ['▁ne', 162], ['▁sh', 163], ['▁ri', 164], ['ry', 165], ['▁ca', 166], ['▁fr', 167], ['▁bo', 168], ['▁r', 169], ['▁kn', 170], ['▁good', 171], ['fe', 172], ['▁com', 173], ['▁j', 174], ['▁now', 175], ['▁up', 176], ['ill', 177], ['ter', 178], ['▁un', 179], ['▁she', 180], ['ster', 181], ['▁de', 182], ['ct', 183], ['ous', 184], ['▁there', 185], ['one', 186], ['ius', 187], ['▁at', 188], ['▁sir', 189], ['▁mar', 190], ['▁mu', 191], ['▁pr', 192], ['▁come', 193], ['▁or', 194], ['la', 195], ['▁du', 196], ['ant', 197], ['ul', 198], ['▁q', 199], ['▁con', 200], ['▁from', 201], ['▁here', 202], ['me', 203], ['sel', 204], ['art', 205], ['▁than', 206], ['▁then', 207], ['ine', 208], ['har', 209], ['▁man', 210], ['▁let', 211], ['▁more', 212], ['▁which', 213], ['▁am', 214], ['ight', 215], ['il', 216], ['▁was', 217], ['own', 218], ['▁would', 219], ['ore', 220], ['▁pro', 221], ['ble', 222], ['▁they', 223], ['▁can', 224], ['▁how', 225], ['out', 226], ['▁dea', 227], ['▁say', 228], ['ven', 229], ['▁ta', 230], ['▁well', 231], ['res', 232], ['▁see', 233], ['▁them', 234], ['▁sw', 235], ['ive', 236], ['ward', 237], ['▁love', 238], ['▁their', 239], ['ic', 240], ['ol', 241], ['ate', 242], ['▁po', 243], ['▁bl', 244], ['▁duke', 245], ['ep', 246], ['ort', 247], ['▁who', 248], ['ure', 249], ['▁ar', 250], ['self', 251], ['▁when', 252], ['ather', 253], ['▁know', 254], ['▁may', 255], ['▁hath', 256], ['pp', 257], ['▁war', 258], ['--', 259], ['un', 260], ['▁where', 261], ['▁us', 262], ['um', 263], ['rie', 264], ['▁gre', 265], ['▁hen', 266], ['ers', 267], ['▁ho', 268], ['ie', 269], ['▁one', 270], ['▁wor', 271], ['ond', 272], ['▁were', 273], ['ame', 274], ['▁en', 275], ['pe', 276], ['▁sa', 277], ['char', 278], ['ot', 279], ['▁que', 280], ['▁make', 281], ['nds', 282], ['▁ba', 283], ['▁upon', 284], ['ink', 285], ['ong', 286], ['fore', 287], ['▁mad', 288], ['▁ag', 289], ['▁should', 290], ['▁le', 291], ['▁like', 292], ['▁did', 293], ['chard', 294], ['▁richard', 295], ['ook', 296], ['▁yet', 297], ['▁tru', 298], ['▁had', 299], ['▁queen', 300], ['▁la', 301], ['▁rome', 302], ['▁fir', 303], ['▁nor', 304], ['ves', 305], ['▁must', 306], ['▁gra', 307], ['▁men', 308], ['▁gl', 309], ['ig', 310], ['ng', 311], ['cent', 312], ['▁some', 313], ['tis', 314], ['▁first', 315], ['ick', 316], ['▁ed', 317], ['▁why', 318], ['row', 319], ['▁too', 320], ['ady', 321], ['▁father', 322], ['pt', 323], ['▁ti', 324], ['▁god', 325], ['▁edward', 326], ['▁again', 327], ['ind', 328], ['eak', 329], ['centio', 330], ['▁ju', 331], ['▁speak', 332], ['ist', 333], ['▁death', 334], ['ish', 335], ['▁bro', 336], ['▁out', 337], ['ren', 338], ['ful', 339], ['ice', 340], ['ince', 341], ['▁hast', 342], ['ff', 343], ['ab', 344], ['de', 345], ['▁gent', 346], ['oun', 347], ['lo', 348], ['ity', 349], ['▁br', 350], ['sed', 351], ['▁give', 352], ['▁take', 353], ['▁henry', 354], ['el', 355], ['ork', 356], ['▁pa', 357], ['em', 358], ['ound', 359], ['▁ab', 360], ['ence', 361], ['▁look', 362], ['▁york', 363], ['▁these', 364], ['are', 365], ['▁ex', 366], ['▁al', 367], ['▁gentle', 368], ['man', 369], ['▁romeo', 370], ['▁mine', 371], ['▁heart', 372], ['▁such', 373], ['▁ser', 374], ['▁tell', 375], ['▁dis', 376], ['▁son', 377], ['ss', 378], ['▁lea', 379], ['ings', 380], ['▁hea', 381], ['▁cor', 382], ['ance', 383], ['▁hear', 384], ['cester', 385], ['▁glou', 386], ['▁gloucester', 387], ['ci', 388], ['▁ro', 389], ['▁off', 390], ['▁time', 391], ['▁brother', 392], ['▁pl', 393], ['▁ear', 394], ['▁lu', 395], ['▁lady', 396], ['▁think', 397], ['wn', 398], ['to', 399], ['ign', 400], ['▁pet', 401], ['ont', 402], ['▁aw', 403], ['ber', 404], ['ness', 405], ['▁cla', 406], ['▁most', 407], ['▁blood', 408], ['est', 409], ['ire', 410], ['▁bre', 411], ['▁des', 412], ['▁ever', 413], ['orn', 414], ['onour', 415], ['ies', 416], ['▁day', 417], ['▁per', 418], ['ran', 419], ['ment', 420], ['▁life', 421], ['ery', 422], ['▁ey', 423], ['▁ii', 424], ['▁frie', 425], ['▁word', 426], ['land', 427], ['▁vin', 428], ['wick', 429], ['▁vincentio', 430], ['per', 431], ['▁art', 432], ['oul', 433], ['▁bu', 434], ['▁warwick', 435], ['▁tw', 436], ['▁fl', 437], ['ught', 438], ['iz', 439], ['chi', 440], ['rit', 441], ['▁fear', 442], ['ose', 443], ['ling', 444], ['ving', 445], ['▁made', 446], ['▁pray', 447], ['ft', 448], ['ian', 449], ['▁ad', 450], ['▁jo', 451], ['▁call', 452], ['▁qu', 453], ['▁bet', 454], ['▁much', 455], ['▁mer', 456], ['urn', 457], ['▁honour', 458], ['eet', 459], ['▁never', 460], ['▁sec', 461], ['use', 462], ['iol', 463], ['age', 464], ['▁stand', 465], ['che', 466], ['ca', 467], ['lt', 468], ['▁ra', 469], ['inius', 470], ['op', 471], ['▁great', 472], ['▁sweet', 473], ['nder', 474], ['▁doth', 475], ['▁heaven', 476], ['▁', 477], ['e', 478], ['t', 479], ['o', 480], ['a', 481], ['i', 482], ['h', 483], ['s', 484], ['r', 485], ['n', 486], ['l', 487], ['d', 488], ['u', 489], ['m', 490], ['y', 491], ['w', 492], [',', 493], ['c', 494], ['f', 495], ['g', 496], ['b', 497], ['p', 498], [':', 499], ['k', 500], ['v', 501], ['.', 502], [\"'\", 503], [';', 504], ['?', 505], ['!', 506], ['-', 507], ['j', 508], ['q', 509], ['x', 510], ['z', 511]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input = \"you are all resolved rather to die than to famish? before we proceed any further, hear me speak.\"\n",
        "ids = sp.encode(input)\n",
        "ids_lower = sp_lower.encode(input)\n",
        "\n",
        "print(len(input))\n",
        "\n",
        "print(len(ids))\n",
        "print([sp.id_to_piece(idx) for idx in ids])\n",
        "\n",
        "print(len(ids_lower))\n",
        "print([sp_lower.id_to_piece(idx) for idx in ids_lower])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_8WqofZnU54",
        "outputId": "92bf7593-0f86-47c8-ee77-8858c64ff388"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "96\n",
            "36\n",
            "['▁you', '▁are', '▁all', '▁re', 's', 'ol', 've', 'd', '▁r', 'ather', '▁to', '▁d', 'ie', '▁than', '▁to', '▁f', 'am', 'ish', '?', '▁be', 'fore', '▁we', '▁pro', 'ce', 'ed', '▁an', 'y', '▁f', 'ur', 't', 'her', ',', '▁hear', '▁me', '▁speak', '.']\n",
            "35\n",
            "['▁you', '▁are', '▁all', '▁re', 's', 'ol', 've', 'd', '▁r', 'ather', '▁to', '▁d', 'ie', '▁than', '▁to', '▁f', 'am', 'ish', '?', '▁be', 'fore', '▁we', '▁pro', 'ce', 'ed', '▁an', 'y', '▁f', 'ur', 'ther', ',', '▁hear', '▁me', '▁speak', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JrmKDiLeo18U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attention Mechanism"
      ],
      "metadata": {
        "id": "nXtcTuMtWcpc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Uniform Attention Mechanism\n",
        "\n",
        "### Introduction\n",
        "When processing a sequence of tokens, it is essential to consider the context provided by past tokens. The order in which tokens appear carries meaningful information. However, in a naive approach, each token is processed independently, meaning there is no interaction between them. Our goal is to allow each token to communicate selectively with past tokens while ensuring that future tokens remain inaccessible, as they are yet to be predicted.\n",
        "\n",
        "### Input Representation\n",
        "We define our input tensor as:\n",
        "\n",
        "```\n",
        "X.shape = (B, T, C) = (4, 8, 32)\n",
        "```\n",
        "where:\n",
        "- **B** is the batch size,\n",
        "- **T** is the number of tokens in the context window,\n",
        "- **C** is the number of channels representing each token.\n",
        "\n",
        "For instance, in a batch, we have **8 tokens**, each described by **32 channels**. Initially, these tokens do not interact with one another, meaning that the model processes them independently. Our objective is to enable communication among tokens while restricting each token’s access to **only previous tokens and itself** (causal attention). This ensures that we do not leak future information, which is crucial for autoregressive models.\n",
        "\n",
        "### Naive Approach: Uniform Averaging\n",
        "A simple way to incorporate past information is to take the average of all accessible past tokens. For example, if **t=5**, we could compute the average representation of tokens **1, 2, 3, 4, 5**. However, this approach is inefficient because it treats all past tokens **equally**, ignoring the varying degrees of relevance different tokens might have.\n",
        "\n",
        "### Weighted Attention with a Masked Matrix\n",
        "We introduce a **weight tensor** that assigns uniform importance levels to past tokens. The weight matrix is designed such that each token can only attend to past tokens and itself while ignoring future tokens.\n",
        "\n",
        "#### Step 1: Creating the Masked Weight Matrix\n",
        "We construct a **lower triangular matrix** that masks out future tokens:\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "T = 8  # Number of tokens in the context window\n",
        "tril = torch.tril(torch.ones(T, T))  # Lower triangular matrix\n",
        "\n",
        "weight = torch.zeros((T, T))  # Initialize weights to zero\n",
        "weight = weight.masked_fill(tril == 0, float('-inf'))  # Mask future tokens\n",
        "weight = F.softmax(weight, dim=-1)  # Apply softmax to get probabilities\n",
        "```\n",
        "\n",
        "- The **lower triangular matrix** ensures that each token only considers past tokens.\n",
        "- Applying **SoftMax** normalizes the values so that the sum of weights for each row equals **1**.\n",
        "- Any masked positions (future tokens) are assigned **-inf**, which results in a softmax output of **zero**, effectively preventing any influence from future tokens.\n",
        "\n",
        "#### Step 2: Applying the Attention Weights\n",
        "Once we have the weight matrix, we apply it to our input tensor `X`:\n",
        "\n",
        "```python\n",
        "output = weight @ X  # (T, T) @ (B, T, C) --> (B, T, C)\n",
        "```\n",
        "This operation aggregates the past information for each token in a weighted manner.\n",
        "\n",
        "### Example of the Weight Matrix\n",
        "For **T=8**, an example of the computed weight matrix is:\n",
        "\n",
        "```\n",
        "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
        "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
        "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
        "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
        "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
        "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
        "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
        "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]])\n",
        "```\n",
        "- Each row corresponds to a token in the sequence.\n",
        "- The sum of each row is **1**, ensuring that each token aggregates its past context without altering the total contribution.\n",
        "- The **fifth row (t=5)**, for instance, distributes attention over tokens **1, 2, 3, 4, 5**.\n",
        "\n",
        "### Summary\n",
        "- The **attention mechanism** allows each token to selectively aggregate information from past tokens while ignoring future ones.\n",
        "- A **triangular masking** approach ensures causality by preventing tokens from seeing future information.\n",
        "- **SoftMax normalization** is used to distribute attention weights effectively.\n",
        "- This weight matrix is the same across all samples in a batch since it depends only on the token positions within the context window.\n",
        "\n",
        "This forms the basis for **self-attention mechanisms** used in modern deep learning architectures like **Transformers**.\n",
        "\n"
      ],
      "metadata": {
        "id": "Y5nQiEBHWeM4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# consider the following toy example:\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,2 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0AhmR8JWdsu",
        "outputId": "cf37b480-acff-4b43-d395-93c8a301e67b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# use Softmax\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "print(wei)\n",
        "out = wei @ x\n",
        "print(out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPSoKaZBXfMA",
        "outputId": "4848a5ff-59d9-44a6-f764-033701978007"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
            "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
            "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n",
            "torch.Size([4, 8, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mrX13NBtYG2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding the Self-Attention Mechanism\n",
        "\n",
        "Self-attention is a crucial component in Transformer models, allowing them to understand context by dynamically weighting the importance of different words in a sequence. Let's break it down step by step.\n",
        "\n",
        "#### The Problem: Ambiguity in Word Meaning\n",
        "\n",
        "Each word (token) in a sentence is initially represented as a vector, called an **embedding**, which captures its general meaning. However, words can have different meanings depending on context. For example, the word \"model\" could refer to an **AI model** or a **fashion model**. To resolve this ambiguity, we need a mechanism that considers surrounding words and extracts the correct meaning. This is where **self-attention** comes in—it enables each word to focus more on contextually relevant words.\n",
        "\n",
        "### How Self-Attention Works\n",
        "Self-attention operates using three key vectors derived from each token:\n",
        "- **Query (Q)**: Represents what this word is looking for in others (e.g., a noun might look for its describing adjective).\n",
        "- **Key (K)**: Represents what this word has to offer to others (e.g., an adjective offers descriptive details).\n",
        "- **Value (V)**: Represents the actual content of the word (e.g., the actual meaning of the word).\n",
        "\n",
        "\n",
        "These vectors are generated by multiplying the input embeddings with three learned weight matrices:\n",
        "\n",
        "$$\n",
        "Q = X \\times W_q, \\quad K = X \\times W_k, \\quad V = X \\times W_v\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "- $X$ is the input word embeddings.\n",
        "- $W_q, W_k, W_v$ are learned weight matrices that transform embeddings into their respective Q, K, and V vectors.\n",
        "\n",
        "### Computing Attention Scores\n",
        "\n",
        "The attention mechanism determines **how much each word should focus on others** in the sentence. This is done by computing a **similarity score** between the Query of one word and the Keys of all words:\n",
        "\n",
        "$$\n",
        "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) \\times V\n",
        "$$\n",
        "\n",
        "- The dot product **$QK^T$** gives a similarity score between words.\n",
        "- The result is scaled by $\\sqrt{d_k}$ (where $d_k$ is the dimension of the key vector) to prevent large values from dominating.\n",
        "- **Softmax** ensures that attention weights sum to 1, making them interpretable as probabilities.\n",
        "- The attention scores determine how much of each value (V) contributes to the final representation.\n",
        "\n"
      ],
      "metadata": {
        "id": "q2AhaPw6hq4D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example\n",
        "\n",
        "Let’s break down the sentence **\"The cat sat on the mat\"** and provide realistic representations for **Attention Weights** and **Value (V)** vectors in the context of self-attention. I’ll also explain the role of each word and how they might interact. Since we’re simulating a Transformer-like model, I’ll make some reasonable assumptions about embeddings and attention behavior, keeping it intuitive yet grounded in how such models typically work.\n",
        "\n",
        "### Setup and Assumptions\n",
        "- **Sentence:** \"The cat sat on the mat\" (6 tokens, including a period for simplicity).\n",
        "- **Vector Size:** Let’s assume 4-dimensional vectors for Q, K, and V (in reality, they’re often 64 or 512 dims, but 4 keeps it manageable).\n",
        "- **Process:** Each word gets Q, K, and V vectors. Attention weights come from Q and K interactions, and V vectors are combined using those weights.\n",
        "- **Goal:** Show how V contributes to the output and interpret each word’s role.\n",
        "\n",
        "I’ll assign plausible V vectors based on linguistic roles (e.g., nouns, verbs, articles) and estimate attention weights based on likely dependencies in this simple sentence.\n",
        "\n",
        "---\n",
        "\n",
        "### Step 1: Define Roles of Each Word\n",
        "Before diving into numbers, let’s understand what each word does:\n",
        "1. **\"The\" (determiner):** Specifies the noun \"cat.\" It’s a helper, pointing to something definite.\n",
        "2. **\"cat\" (noun, subject):** The main actor, the entity doing the sitting.\n",
        "3. **\"sat\" (verb):** The action, central to the sentence, linking \"cat\" to its location.\n",
        "4. **\"on\" (preposition):** Indicates position, connecting \"sat\" to \"mat.\"\n",
        "5. **\"the\" (determiner):** Specifies \"mat,\" another helper for the object.\n",
        "6. **\"mat\" (noun, object):** The location or recipient of the action.\n",
        "\n",
        "In self-attention, each word looks at all others to build context. For example, \"cat\" might focus on \"sat\" (its action) and \"The\" (its specifier), while \"sat\" might focus on \"cat\" (subject) and \"on\" (where it happened).\n",
        "\n",
        "---\n",
        "\n",
        "### Step 2: Assign Plausible Value (V) Vectors\n",
        "These are hypothetical but inspired by how embeddings capture meaning (e.g., nouns might emphasize entity features, verbs action features), consider that V calculates based on input X and learned weights Wv. Let’s use a 4D vector where dimensions loosely represent:\n",
        "- [entity, action, relation, modifier] (just for intuition, not strict).\n",
        "\n",
        "- **\"The\" (first):** V = [0.5, 0.0, 0.2, 0.3]  \n",
        "  - Mild entity boost (specifies something), some relation/modifier role.\n",
        "- **\"cat\":** V = [1.0, 0.1, 0.2, 0.0]  \n",
        "  - Strong entity (noun), tiny action/relation hint.\n",
        "- **\"sat\":** V = [0.2, 1.0, 0.3, 0.0]  \n",
        "  - Strong action (verb), some entity/relation from subject and object.\n",
        "- **\"on\":** V = [0.1, 0.2, 1.0, 0.1]  \n",
        "  - Strong relation (preposition), minor entity/action.\n",
        "- **\"the\" (second):** V = [0.5, 0.0, 0.2, 0.3]  \n",
        "  - Same as first \"The,\" specifying \"mat.\"\n",
        "- **\"mat\":** V = [1.0, 0.1, 0.3, 0.0]  \n",
        "  - Strong entity (noun), slight relation hint (it’s the object).\n",
        "\n",
        "These V vectors are the \"content\" each word offers—its semantic contribution.\n",
        "\n",
        "---\n",
        "\n",
        "### Step 3: Estimate Attention Weights\n",
        "Attention weights come from $$\\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)\n",
        "$$ also Q and K calculated based on input X learned wights of Wq and Wk. We’ll estimate weights based on linguistic intuition. Each row sums to 1.0, representing how much each word attends to others.\n",
        "\n",
        "| **Query Word** | **The1** | **cat** | **sat** | **on** | **the2** | **mat** |\n",
        "|----------------|----------|---------|---------|--------|----------|---------|\n",
        "| **The1**       | 0.40     | 0.40    | 0.10    | 0.05   | 0.03     | 0.02    |\n",
        "| **cat**        | 0.30     | 0.20    | 0.40    | 0.05   | 0.03     | 0.02    |\n",
        "| **sat**        | 0.05     | 0.35    | 0.25    | 0.25   | 0.05     | 0.05    |\n",
        "| **on**         | 0.02     | 0.05    | 0.35    | 0.20   | 0.15     | 0.23    |\n",
        "| **the2**       | 0.03     | 0.02    | 0.05    | 0.15   | 0.35     | 0.40    |\n",
        "| **mat**        | 0.02     | 0.05    | 0.10    | 0.25   | 0.30     | 0.28    |\n",
        "\n",
        "#### Interpretation of Weights:\n",
        "- **\"The1\":** Focuses on itself (0.40) and \"cat\" (0.40), as it specifies the subject.\n",
        "- **\"cat\":** Attends to \"sat\" (0.40) for its action and \"The1\" (0.30) for specificity.\n",
        "- **\"sat\":** Balances \"cat\" (0.35, subject), itself (0.25), and \"on\" (0.25, location).\n",
        "- **\"on\":** Looks at \"sat\" (0.35, the action it modifies) and \"mat\" (0.23, its object).\n",
        "- **\"the2\":** Focuses on \"mat\" (0.40) and itself (0.35), specifying the object.\n",
        "- **\"mat\":** Attends to \"on\" (0.25, its position) and \"the2\" (0.30, its specifier).\n",
        "\n",
        "These weights reflect dependency-like relationships, close to what a trained model might learn.\n",
        "\n",
        "---\n",
        "\n",
        "### Step 4: Compute Output for One Word (Example: \"cat\")\n",
        "Using the attention weights for \"cat\" and the V vectors:\n",
        "- Weights: [0.30, 0.20, 0.40, 0.05, 0.03, 0.02]\n",
        "- V vectors: From above.\n",
        "\n",
        "Output = Σ (weight × V):\n",
        "- 0.30 × [0.5, 0.0, 0.2, 0.3] = [0.15, 0.0, 0.06, 0.09] (The1)\n",
        "- 0.20 × [1.0, 0.1, 0.2, 0.0] = [0.20, 0.02, 0.04, 0.0] (cat)\n",
        "- 0.40 × [0.2, 1.0, 0.3, 0.0] = [0.08, 0.40, 0.12, 0.0] (sat)\n",
        "- 0.05 × [0.1, 0.2, 1.0, 0.1] = [0.005, 0.01, 0.05, 0.005] (on)\n",
        "- 0.03 × [0.5, 0.0, 0.2, 0.3] = [0.015, 0.0, 0.006, 0.009] (the2)\n",
        "- 0.02 × [1.0, 0.1, 0.3, 0.0] = [0.02, 0.002, 0.006, 0.0] (mat)\n",
        "\n",
        "Sum = [0.47, 0.432, 0.292, 0.104]\n",
        "\n",
        "**New \"cat\" representation:** [0.47, 0.432, 0.292, 0.104]\n",
        "- Stronger entity (0.47) and action (0.432) from \"cat\" and \"sat,\" with some relation (0.292) from \"on.\"\n",
        "\n",
        "---\n",
        "\n",
        "### Step 5: Role of Each Word via V and Attention\n",
        "- **\"The1\":** Its V ([0.5, 0.0, 0.2, 0.3]) gives \"cat\" specificity (entity + modifier). Attention to \"cat\" (0.40) reinforces this.\n",
        "- **\"cat\":** Its V ([1.0, 0.1, 0.2, 0.0]) is the subject’s core. It pulls action from \"sat\" (0.40 weight).\n",
        "- **\"sat\":** Its V ([0.2, 1.0, 0.3, 0.0]) is the action hub, shared with \"cat\" and \"on,\" shaping the event.\n",
        "- **\"on\":** Its V ([0.1, 0.2, 1.0, 0.1]) links \"sat\" to \"mat,\" adding relational context.\n",
        "- **\"the2\":** Its V ([0.5, 0.0, 0.2, 0.3]) specifies \"mat,\" aiding the object’s clarity.\n",
        "- **\"mat\":** Its V ([1.0, 0.1, 0.3, 0.0]) is the object, enriched by \"on\" and \"the2.\"\n",
        "\n",
        "---\n",
        "\n",
        "### Reality Check\n",
        "In a real Transformer:\n",
        "- Vectors would be higher-dimensional (e.g., 512), capturing nuanced features.\n",
        "- Attention weights would come from trained Q-K dot products, not intuition.\n",
        "- Multi-head attention would split focus across different aspects (e.g., syntax vs. semantics).\n",
        "\n",
        "Still, this approximation mirrors how V carries meaning (e.g., action from \"sat\") and attention weights prioritize relationships (e.g., \"cat\" to \"sat\").\n",
        "\n",
        "---\n",
        "\n",
        "### Closing\n",
        "The V vectors are the \"what\" each word contributes—its semantic essence—while attention weights decide \"how much\" of each V matters. For \"cat,\" the output blends its identity with its action (\"sat\"), tuned by helpers like \"The\" and \"on.\" Does this bring it closer to reality for you? Want to tweak something or explore another word’s output? Let me know!"
      ],
      "metadata": {
        "id": "47cdq__NjzBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# self-attention!\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "# let's see a single Head perform self-attention\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "k = key(x)   # (B, T, 16)\n",
        "q = query(x) # (B, T, 16)\n",
        "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "#wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "v = value(x)\n",
        "out = wei @ v\n",
        "\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2_uldCiiISr",
        "outputId": "70cbd7c9-20a3-410f-c90f-e66dcf489166"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each linear layer (nn.Linear) transforms the input tensor from the original channel size (C) to a specified head_size without bias. We do this because we want to project the original input tensor x, with dimensions (B, T, C), into three separate spaces—keys (k), queries (q), and values (v)—each with a reduced dimensionality specified by the hyperparameter head_size.\n",
        "\n",
        "**Why We Don’t Use Bias in Self-Attention**\n",
        "*   Each linear layer in self-attention serves only as a projection from input features (C) to the head_size dimension. Unlike activation functions (like ReLU), where bias can help with shifting the output, the self-attention mechanism does not require a bias shift\n",
        "*   Self-attention works on relative importance, making absolute shifts (caused by bias) irrelevant.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Notes:**\n",
        "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
        "- There is no notion of space (awareness of position of the token in the context). Attention simply acts over a set of vectors. This is why we need to positionally encode tokens (which is added to inouts).\n",
        "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
        "- In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling (attention paper has both encoder and decoder and designed for translation not language model).\n",
        "- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
        "- \"Scaled\" attention additional divides `wei` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"
      ],
      "metadata": {
        "id": "M5CvobiQ0pLr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8qB7-4SA1KL0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}