# GPT-2 from Scratch with Byte Pair Encoding (BPE) Tokenizer

## Overview
This repository contains an implementation of a GPT-2-like Language Model (LLM) from scratch, along with an effective tokenizer using Byte Pair Encoding (BPE). The project is deeply focused on understanding each step in the process, following the principles and insights from Andrej Karpathy's educational tutorials.

## Inspiration
This project is inspired by Andrej Karpathy's excellent deep-learning tutorial series on YouTube:
- [Let's build the GPT Tokenizer](https://www.youtube.com/watch?v=zduSFxRajkE&ab_channel=AndrejKarpathy)
- [Let's build GPT: from scratch](https://www.youtube.com/watch?v=kCc8FmEb1nY&ab_channel=AndrejKarpathy)
- [Let's reproduce GPT-2 (124M)](https://www.youtube.com/watch?v=l8pRSuU81PU&ab_channel=AndrejKarpathy)

## Features
- **Byte Pair Encoding (BPE) Tokenizer:** Efficient tokenization of text data using subword units.
- **Transformer Model Implementation:** Builds a GPT-2-like model from the ground up.
- **Step-by-Step Understanding:** Code is structured for clarity and learning.


## Acknowledgments
Special thanks to Andrej Karpathy for his incredible teaching materials and in-depth breakdown of how modern language models work.

