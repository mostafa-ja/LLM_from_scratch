{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPG4xj08Aq09UB+I0AMONxI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mostafa-ja/LLM_from_scratch/blob/main/GPT_2_small_dataset_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "D1QFnSOgaNow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import time\n",
        "import inspect\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from dataclasses import dataclass\n",
        "from tqdm import tqdm\n",
        "from torch.nn.utils import clip_grad_norm_"
      ],
      "metadata": {
        "id": "CI1Y4TySlb1u"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
        "\n",
        "        # regularization\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.register_buffer('bias', torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                                    .view(1,1, config.block_size, config.block_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "        qkv = self.c_attn(x) # shape: (B, T, 3 * C)\n",
        "        q, k, v = qkv.split(self.n_embd, dim=2) # Each of shape (B, T, C)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "\n",
        "        #att = (q @ k.transpose(-2,-1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        #att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float(\"-inf\"))\n",
        "        #att = F.softmax(att, dim=-1)\n",
        "        #y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "\n",
        "        # flashattention\n",
        "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
        "\n",
        "        y = y.transpose(1,2).contiguous().view(B, T, C) # (B, nh, T, hs) → (B, T, nh, hs) → (B, T, nh * hs) = (B, T, C)\n",
        "        y = self.c_proj(y)\n",
        "\n",
        "        return y\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
        "        self.gelu    = nn.GELU(approximate='tanh')\n",
        "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd)\n",
        "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = nn.LayerNorm(config.n_embd)\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "        # weight sharing scheme\n",
        "        self.transformer[\"wte\"].weight = self.lm_head.weight\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            std = 0.02\n",
        "            if hasattr(module, 'NANOGPT_SCALE_INIT'):\n",
        "                std *= (2 * self.config.n_layer) ** -0.5\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        # idx is of shape (B, T)\n",
        "        B, T = idx.size()\n",
        "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
        "\n",
        "        # forward the token and position embeddings\n",
        "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device).unsqueeze(0)  # shape (1, T)\n",
        "        pos_emb = self.transformer[\"wpe\"](pos)               # (1, T, n_embd)\n",
        "        tok_emb = self.transformer[\"wte\"](idx)               # (B, T, n_embd)\n",
        "        x = tok_emb + pos_emb                                # broadcasting (B, T, n_embd)\n",
        "\n",
        "        # forward the blocks of the transformer\n",
        "        for block in self.transformer[\"h\"]:\n",
        "            x = block(x)\n",
        "\n",
        "        # final layernorm and output projection\n",
        "        x = self.transformer[\"ln_f\"](x)\n",
        "        logits = self.lm_head(x)                             # (B, T, vocab_size)\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def configure_optimizers(self, weight_decay, learning_rate, device_type):\n",
        "\n",
        "        # Select parameters that require gradients\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters() if p.requires_grad}\n",
        "\n",
        "        # Separate parameters for weight decay\n",
        "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "        optim_groups = [\n",
        "            {'params': decay_params, 'weight_decay': weight_decay},\n",
        "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "        ]\n",
        "\n",
        "        # Display parameter distribution\n",
        "        print(f\"decay param tensors: {len(decay_params)}, total: {sum(p.numel() for p in decay_params):,}\")\n",
        "        print(f\"no-decay param tensors: {len(nodecay_params)}, total: {sum(p.numel() for p in nodecay_params):,}\")\n",
        "\n",
        "        # Use fused AdamW if supported and on CUDA\n",
        "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "        use_fused = fused_available and device_type == \"cuda\"\n",
        "        print(f\"Using fused AdamW: {use_fused}\")\n",
        "\n",
        "        optimizer = torch.optim.AdamW(\n",
        "            optim_groups,\n",
        "            lr=learning_rate,\n",
        "            betas=(0.9, 0.95),\n",
        "            eps=1e-8,\n",
        "            fused=use_fused\n",
        "        )\n",
        "        return optimizer\n"
      ],
      "metadata": {
        "id": "wts6F77DloAZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YRMxruvs3s16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "mgwEWb4eaK2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install transformers\n",
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTBVaVFHwxv1",
        "outputId": "a3a6f030-d82f-418d-fc8d-280f9dbe070f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.4.26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tiktoken\n",
        "#from transformers import AutoTokenizer#LLaMA 3 tokenizer"
      ],
      "metadata": {
        "id": "_h9hOzraaWZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tokenizer(model_desc):\n",
        "    \"\"\"Returns tokenizer function and end-of-text token based on model.\"\"\"\n",
        "    if model_desc == \"gpt-2\":\n",
        "        enc = tiktoken.get_encoding(\"gpt2\")\n",
        "        encode = lambda s: enc.encode_ordinary(s)\n",
        "        eot_token = enc._special_tokens['<|endoftext|>']\n",
        "    elif model_desc == \"llama-3\":\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B\")\n",
        "        encode = lambda s: tokenizer.encode(s, add_special_tokens=False, verbose=False, split_special_tokens=True)\n",
        "        eot_token = tokenizer.encode('')[0]  # Adds EOT token by default\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model descriptor: {model_desc}\")\n",
        "    return encode, eot_token\n",
        "\n",
        "def process_text_file(filepath, encode, eot_token, train_split=0.9):\n",
        "    \"\"\"Reads text, tokenizes with EOT prepended to each section, and splits into train/val.\"\"\"\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "\n",
        "    # Splits the text into chunks wherever there are double newlines (\\n\\n), treating each as a separate \"document\" and Remove those \\n\\n separators.\n",
        "    sections = text.split(\"\\n\\n\")\n",
        "    tokens = []\n",
        "\n",
        "    for i, section in enumerate(sections):\n",
        "        tokens.append(eot_token)  # Prepend EOT to mark document start\n",
        "        padded_section = section + \"\\n\\n\" if i != len(sections) - 1 else section\n",
        "        tokens.extend(encode(padded_section))\n",
        "\n",
        "    split_idx = int(train_split * len(tokens))\n",
        "    train_tokens = tokens[:split_idx]\n",
        "    val_tokens = tokens[split_idx:]\n",
        "    return train_tokens, val_tokens"
      ],
      "metadata": {
        "id": "uhWMICSEagN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataLoaderLite:\n",
        "    def __init__(self, tokens, batch_size, block_size):\n",
        "        self.tokens = torch.tensor(tokens, dtype=torch.long)\n",
        "        self.batch_size = batch_size\n",
        "        self.block_size = block_size\n",
        "\n",
        "        self.total_tokens = len(self.tokens)\n",
        "        self.tokens_per_epoch = (self.total_tokens - 1) // (batch_size * block_size)\n",
        "        assert self.tokens_per_epoch > 0, \"Tokens per epoch must be positive\"\n",
        "\n",
        "        # Truncate to a clean multiple\n",
        "        usable_tokens = self.tokens_per_epoch * batch_size * block_size + 1\n",
        "        self.tokens = self.tokens[:usable_tokens]\n",
        "\n",
        "        self.current_position = 0\n",
        "        print(f\"1 epoch in data loader = {self.tokens_per_epoch} batches\")\n",
        "\n",
        "    def next_batch(self):\n",
        "        B, T = self.batch_size, self.block_size\n",
        "        start = self.current_position\n",
        "        end = start + B * T + 1\n",
        "\n",
        "        if end > len(self.tokens):\n",
        "            self.current_position = 0\n",
        "            start, end = 0, B * T + 1\n",
        "\n",
        "        buf = self.tokens[start:end]\n",
        "        x = buf[:-1].view(B, T)\n",
        "        y = buf[1:].view(B, T)\n",
        "\n",
        "        self.current_position += B * T\n",
        "        return x, y\n",
        "\n",
        "    def __next__(self):\n",
        "        if self.current_position + self.batch_size * self.block_size + 1 > len(self.tokens):\n",
        "            print('Next epoch ....')\n",
        "        return self.next_batch()\n"
      ],
      "metadata": {
        "id": "lCiZ9PHJds8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "q9q60OrFdFZl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== Set Seed for Reproducibility ====\n",
        "def set_seed(seed=1337):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)"
      ],
      "metadata": {
        "id": "e48JVKSoegTq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def get_lr(step, max_lr = 6e-4, warmup_steps = 10, max_steps= 1000):\n",
        "    \"\"\"\n",
        "    Compute learning rate using linear warmup followed by cosine decay.\n",
        "\n",
        "    Args:\n",
        "        step (int): Current training step.\n",
        "        max_lr (float): Peak learning rate after warmup.\n",
        "        min_lr (float): Final learning rate after decay (default: 10% of max_lr).\n",
        "        warmup_steps (int): Number of steps for linear warmup.\n",
        "        max_steps (int): Total number of training steps.\n",
        "\n",
        "    Returns:\n",
        "        float: Learning rate at the current step.\n",
        "    \"\"\"\n",
        "    min_lr = max_lr * 0.1\n",
        "\n",
        "    if step < warmup_steps:\n",
        "        # Linear warmup\n",
        "        return (max_lr * (step + 1) / warmup_steps)\n",
        "\n",
        "    if step >= max_steps:\n",
        "        # After training ends, return floor LR\n",
        "        return min_lr\n",
        "\n",
        "    # Cosine decay\n",
        "    decay_ratio = (step - warmup_steps) / (max_steps - warmup_steps)\n",
        "    decay_ratio = min(max(decay_ratio, 0.0), 1.0)  # Clamp between [0, 1]\n",
        "    cosine_decay = 0.5 * (1 + math.cos(math.pi * decay_ratio))\n",
        "    lr = min_lr + (max_lr - min_lr) * cosine_decay\n",
        "    return lr"
      ],
      "metadata": {
        "id": "G85FsAoYW5j3"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('dataset', exist_ok=True)\n",
        "!wget -O dataset/tiny_shakespeare.txt https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "\n",
        "model_desc = \"gpt-2\"\n",
        "data_path = os.path.join('dataset', \"tiny_shakespeare.txt\")\n",
        "\n",
        "encode_fn, eot = get_tokenizer(model_desc)\n",
        "train_tokens, val_tokens = process_text_file(data_path, encode_fn, eot)\n",
        "\n"
      ],
      "metadata": {
        "id": "Wbu8mEEKaobD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80f2832d-f860-4c25-ce66-9f3ed541f379"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-07 17:15:57--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘dataset/tiny_shakespeare.txt’\n",
            "\n",
            "\r          dataset/t   0%[                    ]       0  --.-KB/s               \rdataset/tiny_shakes 100%[===================>]   1.06M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2025-05-07 17:15:57 (26.4 MB/s) - ‘dataset/tiny_shakespeare.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 1024 # max sequence length\n",
        "    vocab_size: int = 50304 # number of tokens, instead of 50257\n",
        "    n_layer: int = 12 # number of transformer layers\n",
        "    n_head: int = 12 # number of heads in multi-head attention\n",
        "    n_embd: int = 768 # embedding dimension"
      ],
      "metadata": {
        "id": "3ndR8bb4ltWg"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# ==== Initialize ====\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "torch.set_float32_matmul_precision('high')\n",
        "print(f\"Current precision: {torch.get_float32_matmul_precision()}\")\n",
        "set_seed(1337)\n",
        "\n",
        "\n",
        "total_batch_size = 524288  # 2**19 ~ 0.5M used for nice number\n",
        "T = 1024  # Sequence length   (real is 1024)\n",
        "B = 4  # Micro batch size (real is 16)\n",
        "max_lr = 6e-4\n",
        "max_steps = 50\n",
        "\n",
        "assert total_batch_size % (B * T) == 0, \"Total batch size must be divisible by micro batch size and sequence length\"\n",
        "grad_accum_steps = total_batch_size // (B * T)\n",
        "print(f\"Total batch size: {total_batch_size}\")\n",
        "print(f\"Gradient accumulation steps: {grad_accum_steps}\")\n",
        "\n",
        "train_loader = DataLoaderLite(train_tokens, B, T)\n",
        "val_loader = DataLoaderLite(val_tokens, B, T)\n",
        "\n",
        "\n",
        "# ==== Model and Optimizer ====\n",
        "model = GPT(GPTConfig()).to(device)\n",
        "model = torch.compile(model)\n",
        "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "optimizer = model.configure_optimizers(\n",
        "    weight_decay=0.1,\n",
        "    learning_rate=max_lr,\n",
        "    device_type=device\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "model.train()\n",
        "for step in range(max_steps):\n",
        "    start_time = time.time()\n",
        "    optimizer.zero_grad()\n",
        "    loss_accum = 0.0\n",
        "    for micro_batch in range(grad_accum_steps):\n",
        "        x, y = train_loader.next_batch()\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        # Use autocast to automatically cast the model to the correct precision\n",
        "        with torch.autocast(device, dtype=torch.bfloat16):\n",
        "            logits, loss = model(x, y)\n",
        "\n",
        "        # We need to scale our loss by the number of micro batches to get the correct gradient\n",
        "        loss = loss / grad_accum_steps\n",
        "        loss_accum += loss.detach()\n",
        "        # By keeping the loss in the loop, we can accumulate the gradients over the micro batches\n",
        "        loss.backward()\n",
        "\n",
        "    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    lr = get_lr(step, max_lr=max_lr, warmup_steps=10, max_steps=max_steps) #int(max_steps * 0.01)\n",
        "    for group in optimizer.param_groups:\n",
        "        group[\"lr\"] = lr\n",
        "\n",
        "    # Outside the microbatch loop so all accumulated update in one go\n",
        "    optimizer.step()\n",
        "    torch.cuda.synchronize()\n",
        "    elapsed = time.time() - start_time\n",
        "    tokens_per_sec = (B * T * grad_accum_steps) / elapsed\n",
        "    print(f\"Step {step + 1:5d} | Loss: {loss_accum:.4f} | LR: {lr:.2e} | GradNorm: {grad_norm:.2f} | Tokens/sec: {tokens_per_sec:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezDmmMF6gQiL",
        "outputId": "03a6ccb2-6be0-4588-b480-8b0778f65ae1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Current precision: high\n",
            "Total batch size: 524288\n",
            "Gradient accumulation steps: 128\n",
            "1 epoch in data loader = 74 batches\n",
            "1 epoch in data loader = 8 batches\n",
            "Total parameters: 124,475,904\n",
            "decay param tensors: 50, total: 124,354,560\n",
            "no-decay param tensors: 98, total: 121,344\n",
            "Using fused AdamW: True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "W0507 17:17:40.584000 12417 torch/_inductor/utils.py:1137] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_inductor/compile_fx.py:1948: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_inductor/compile_fx.py:1948: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_inductor/compile_fx.py:1948: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_inductor/compile_fx.py:1948: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step     1 | Loss: 10.9876 | LR: 6.00e-05 | GradNorm: 2.36 | Tokens/sec: 2463.78\n",
            "Step     2 | Loss: 10.3814 | LR: 1.20e-04 | GradNorm: 2.52 | Tokens/sec: 2588.20\n",
            "Step     3 | Loss: 9.6795 | LR: 1.80e-04 | GradNorm: 2.53 | Tokens/sec: 2565.79\n"
          ]
        }
      ]
    }
  ]
}