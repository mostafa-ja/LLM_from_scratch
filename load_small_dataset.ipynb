{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPWrFS8sXpJyHYweacX8Wwo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mostafa-ja/LLM_from_scratch/blob/main/load_small_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTBVaVFHwxv1",
        "outputId": "10b62832-4060-468e-dc38-860a406d9ac5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.4.26)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement data_common (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for data_common\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "D49c2AAawh2Z"
      },
      "outputs": [],
      "source": [
        "import argparse, os\n",
        "import tiktoken\n",
        "from transformers import AutoTokenizer #LLaMA 3 tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`encode_ordinary(s)` is a method from the **`tiktoken`** library used specifically for GPT-style tokenization.\n",
        "\n",
        "---\n",
        "\n",
        "> üîç What It Does\n",
        "\n",
        "```python\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "enc.encode_ordinary(s)\n",
        "```\n",
        "\n",
        "This encodes the input string `s` into a list of **token IDs**, but **without** adding any *special tokens* like:\n",
        "\n",
        "* `<|endoftext|>` (EOT)\n",
        "* Padding or BOS/EOS tokens (if used in other contexts)\n",
        "\n",
        "---\n",
        "\n",
        "> ‚úÖ Example\n",
        "\n",
        "```python\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "enc.encode_ordinary(\"Hello world\")\n",
        "# Output: [15496, 995]\n",
        "```\n",
        "\n",
        "* `15496` = \"Hello\"\n",
        "* `995` = \" world\" (note the space)\n",
        "\n",
        "Now compare that with a tokenizer that **adds special tokens**:\n",
        "\n",
        "```python\n",
        "enc.encode(\"Hello world\")\n",
        "# Output: [15496, 995, 50256]\n",
        "```\n",
        "\n",
        "* `50256` = `<|endoftext|>` (automatically appended)\n",
        "\n",
        "So:\n",
        "\n",
        "* `encode_ordinary(s)` ‚Üí *pure* tokenization of the text, no extras.\n",
        "* `encode(s)` ‚Üí might add special tokens (like EOT) depending on settings.\n",
        "\n",
        "---\n",
        "\n",
        "> üîß Why Use `encode_ordinary`?\n",
        "\n",
        "In many training pipelines, especially when you're manually managing formatting (like inserting EOTs), you want **full control**, so `encode_ordinary` is preferred.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "s_oxmKG115j8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's a **minimal example** showing how token sequences differ **with** and **without** the EOT token.\n",
        "\n",
        "---\n",
        "\n",
        "> ‚öô Setup\n",
        "\n",
        "We'll compare tokenized output for two short \"documents\" using the GPT-2 tokenizer via `tiktoken`:\n",
        "\n",
        "```python\n",
        "import tiktoken\n",
        "\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "eot = enc._special_tokens['<|endoftext|>']\n",
        "\n",
        "doc1 = \"To be or not to be.\"\n",
        "doc2 = \"That is the question.\"\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "> üß™ 1. **Without EOT**\n",
        "\n",
        "```python\n",
        "tokens = enc.encode_ordinary(doc1) + enc.encode_ordinary(doc2)\n",
        "print(tokens)\n",
        "```\n",
        "\n",
        "**Output:**\n",
        "\n",
        "```\n",
        "[539, 389, 329, 703, 539, 13, 1804, 318, 262, 1123, 13]\n",
        "```\n",
        "\n",
        "* This is just the two texts glued together.\n",
        "* The model may treat them as part of **one coherent sentence**, even though they're conceptually separate.\n",
        "\n",
        "---\n",
        "\n",
        "> üß™ 2. **With EOT Between Documents**\n",
        "\n",
        "```python\n",
        "tokens = [eot] + enc.encode_ordinary(doc1) + [eot] + enc.encode_ordinary(doc2)\n",
        "print(tokens)\n",
        "```\n",
        "\n",
        "**Output (example):**\n",
        "\n",
        "```\n",
        "[50256, 539, 389, 329, 703, 539, 13, 50256, 1804, 318, 262, 1123, 13]\n",
        "```\n",
        "\n",
        "Here:\n",
        "\n",
        "* `50256` is the EOT token for GPT-2.\n",
        "* The model now sees two **clearly separated** text segments:\n",
        "\n",
        "  * `[EOT] To be or not to be.`\n",
        "  * `[EOT] That is the question.`\n",
        "\n",
        "This helps the model reset context and avoid blending unrelated documents.\n",
        "\n",
        "---\n",
        "\n",
        "> üß† Why It Matters\n",
        "\n",
        "Without EOT:\n",
        "\n",
        "* The model may predict `\"That\"` as a continuation of `\"To be or not to be.\"`\n",
        "\n",
        "With EOT:\n",
        "\n",
        "* The model is less likely to do that, since it sees a clear **document boundary**.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "3m_araAs42Dw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ü§î Why is the EOT placed **at the beginning** of each document (instead of at the end)?\n",
        "\n",
        "```python\n",
        "tokens.append(eot)  # prepend EOT\n",
        "tokens.extend(encode(spad))  # then add the actual text\n",
        "```\n",
        "\n",
        "> üß† The Reason: **Training with autoregressive language models (like GPT-2)**\n",
        "\n",
        "In **causal language modeling**, models are trained to predict the **next token** given all previous tokens.\n",
        "\n",
        "That means:\n",
        "\n",
        "* At training time, each token is predicted based on everything **to its left**.\n",
        "* So, what you **put before** a sentence matters most.\n",
        "\n",
        "---\n",
        "\n",
        "> ‚úÖ Why EOT at the Beginning Works\n",
        "\n",
        "Putting an EOT **before** a new sentence acts as a **context reset**:\n",
        "\n",
        "* It signals: ‚ÄúWe‚Äôre starting a fresh, new document now.‚Äù\n",
        "* The model learns: **\"Whenever I see `<|endoftext|>`, forget what came before ‚Äî I'm starting something new.\"**\n",
        "\n",
        "> ‚ùå Why EOT at the End Isn't Enough\n",
        "\n",
        "If you only put EOT **after** the sentence, it comes **too late**:\n",
        "\n",
        "* The model would process the whole sentence **without knowing** it‚Äôs a new doc.\n",
        "* It can carry over context from previous sentences ‚Äî especially in long sequences.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "yEwOpQtS5KSH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tokenizer(model_desc):\n",
        "    \"\"\"Returns tokenizer function and end-of-text token based on model.\"\"\"\n",
        "    if model_desc == \"gpt-2\":\n",
        "        enc = tiktoken.get_encoding(\"gpt2\")\n",
        "        encode = lambda s: enc.encode_ordinary(s)\n",
        "        eot_token = enc._special_tokens['<|endoftext|>']\n",
        "    elif model_desc == \"llama-3\":\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B\")\n",
        "        encode = lambda s: tokenizer.encode(s, add_special_tokens=False, verbose=False, split_special_tokens=True)\n",
        "        eot_token = tokenizer.encode('')[0]  # Adds EOT token by default\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model descriptor: {model_desc}\")\n",
        "    return encode, eot_token\n",
        "\n",
        "def process_text_file(filepath, encode, eot_token, train_split=0.9):\n",
        "    \"\"\"Reads text, tokenizes with EOT prepended to each section, and splits into train/val.\"\"\"\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "\n",
        "    # Splits the text into chunks wherever there are double newlines (\\n\\n), treating each as a separate \"document\" and Remove those \\n\\n separators.\n",
        "    sections = text.split(\"\\n\\n\")\n",
        "    tokens = []\n",
        "\n",
        "    for i, section in enumerate(sections):\n",
        "        tokens.append(eot_token)  # Prepend EOT to mark document start\n",
        "        padded_section = section + \"\\n\\n\" if i != len(sections) - 1 else section\n",
        "        tokens.extend(encode(padded_section))\n",
        "\n",
        "    split_idx = int(train_split * len(tokens))\n",
        "    train_tokens = tokens[:split_idx]\n",
        "    val_tokens = tokens[split_idx:]\n",
        "    return train_tokens, val_tokens\n"
      ],
      "metadata": {
        "id": "KRHwPZIt2AUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('dataset', exist_ok=True)\n",
        "!wget -O dataset/tiny_shakespeare.txt https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "id": "g2veXG3S7ZPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_desc = \"gpt-2\"\n",
        "data_path = os.path.join('dataset', \"tiny_shakespeare.txt\")\n",
        "\n",
        "encode_fn, eot = get_tokenizer(model_desc)\n",
        "train_tokens, val_tokens = process_text_file(data_path, encode_fn, eot)"
      ],
      "metadata": {
        "id": "o6m5RT6W7PFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class DataLoaderLite:\n",
        "    def __init__(self, tokens, batch_size, block_size):\n",
        "        self.tokens = torch.tensor(tokens, dtype=torch.long)\n",
        "        self.batch_size = batch_size\n",
        "        self.block_size = block_size\n",
        "\n",
        "        self.total_tokens = len(self.tokens)\n",
        "        self.tokens_per_epoch = (self.total_tokens - 1) // (batch_size * block_size)\n",
        "        assert self.tokens_per_epoch > 0, \"Tokens per epoch must be positive\"\n",
        "\n",
        "        # Truncate to a clean multiple\n",
        "        usable_tokens = self.tokens_per_epoch * batch_size * block_size + 1\n",
        "        self.tokens = self.tokens[:usable_tokens]\n",
        "\n",
        "        self.current_position = 0\n",
        "        print(f\"1 epoch in data loader = {self.tokens_per_epoch} batches\")\n",
        "\n",
        "    def next_batch(self):\n",
        "        B, T = self.batch_size, self.block_size\n",
        "        start = self.current_position\n",
        "        end = start + B * T + 1\n",
        "\n",
        "        if end > len(self.tokens):\n",
        "            self.current_position = 0\n",
        "            start, end = 0, B * T + 1\n",
        "\n",
        "        buf = self.tokens[start:end]\n",
        "        x = buf[:-1].view(B, T)\n",
        "        y = buf[1:].view(B, T)\n",
        "\n",
        "        self.current_position += B * T\n",
        "        return x, y\n",
        "\n",
        "    def __next__(self):\n",
        "        if self.current_position + self.batch_size * self.block_size + 1 > len(self.tokens):\n",
        "            print('Next epoch ....')\n",
        "        return self.next_batch()\n"
      ],
      "metadata": {
        "id": "lCiZ9PHJds8j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}